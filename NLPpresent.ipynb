{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 생성, 로드, 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Output from Raw Text file-----------\n",
      "\n",
      "Hello NLP!\n",
      "We are DNetSCI LAB members.\n",
      "2\n",
      "['Hello NLP!', 'We are DNetSCI LAB members.']\n",
      "\n",
      "-------Output from assigned variable-------\n",
      "\n",
      " one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
      "\n",
      "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
      "\n",
      "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
      "\n",
      "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\n",
      "6\n",
      "[' one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.', 'It is not easy to include all this information in just a few words.', 'Start by writing a summary that includes whatever you think is important,\\n\\n    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.', \"Don't use abbreviations or citations in the abstract.\", 'It should be able to stand alone without any footnotes.', 'Fig 1.1.1 shows below.']\n",
      "\n",
      "-------Output Corpus data--------------\n",
      "\n",
      "[The Adventures of Buster Bear by Thornton W. Burgess 1920]\n",
      "\n",
      "I\n",
      "\n",
      "BUSTER BEAR GOES FISHING\n",
      "\n",
      "\n",
      "Buster Bear yawned as he lay on his comfortable bed of leaves and\n",
      "watched the first early morning sunbeams creeping through the Green\n",
      "Forest to chase out the Black Shadows. Once more he yawned, and slowly\n",
      "got to his feet and shook himself. Then he walked over to a big\n",
      "pine-tree, stood up on his hind legs, reached as high up on the trunk of\n",
      "the tree as he could, and scratched the bark with his great claws. After\n",
      "that he yawned until it seemed as if his jaws would crack, and then sat\n",
      "down to think what he wanted for breakfast.\n",
      "\n",
      "While he sat there, trying to make up his mind what would taste best, he\n",
      "was listening to the sounds that told of the waking of all the little\n",
      "people who live in the Green Forest. He heard Sammy Jay way off in the\n",
      "distance screaming, \"Thief! Thief!\" and grinned. \"I wonder,\" thought\n",
      "Buster, \"if some one has stolen Sammy's breakfast, or if he has stolen\n",
      "th\n",
      "9\n",
      "['[The Adventures of Buster Bear by Thornton W. Burgess 1920]\\r\\n\\r\\nI\\r\\n\\r\\nBUSTER BEAR GOES FISHING\\r\\n\\r\\n\\r\\nBuster Bear yawned as he lay on his comfortable bed of leaves and\\r\\nwatched the first early morning sunbeams creeping through the Green\\r\\nForest to chase out the Black Shadows.', 'Once more he yawned, and slowly\\r\\ngot to his feet and shook himself.', 'Then he walked over to a big\\r\\npine-tree, stood up on his hind legs, reached as high up on the trunk of\\r\\nthe tree as he could, and scratched the bark with his great claws.', 'After\\r\\nthat he yawned until it seemed as if his jaws would crack, and then sat\\r\\ndown to think what he wanted for breakfast.', 'While he sat there, trying to make up his mind what would taste best, he\\r\\nwas listening to the sounds that told of the waking of all the little\\r\\npeople who live in the Green Forest.', 'He heard Sammy Jay way off in the\\r\\ndistance screaming, \"Thief!', 'Thief!\"', 'and grinned.', '\"I wonder,\" thought\\r\\nBuster, \"if some one has stolen Sammy\\'s breakfast, or if he has stolen\\r\\nth']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 생성, 로드, 문장 토큰화\n",
    "#1.Raw text file\n",
    "#2.Define raw data text\n",
    "#3.Use of corpus from nltk \n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import gutenberg as cg\n",
    "\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get raw data form file\n",
    "\n",
    "def fileread():\n",
    "\n",
    "    file_contents = open(\"corpusData.txt\", \"r\").read()\n",
    "\n",
    "    # print file_contents\n",
    "\n",
    "    return file_contents\n",
    "\n",
    "# assign text data to local variable\n",
    "\n",
    "def localtextvalue():\n",
    "\n",
    "    text = \"\"\" one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
    "\n",
    "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
    "\n",
    "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
    "\n",
    "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\"\"\"\n",
    "\n",
    "    # print text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# Use NLTK corpus which we seen in chapter 2 as well\n",
    "\n",
    "def readcorpus():\n",
    "\n",
    "    raw_content_cg = cg.raw(\"burgess-busterbrown.txt\")\n",
    "\n",
    "    # print raw_content_cg[0:1000]\n",
    "\n",
    "    return raw_content_cg[0:1000]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"----------Output from Raw Text file-----------\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    filecontentdetails = fileread()\n",
    "\n",
    "    print(filecontentdetails)\n",
    "\n",
    "    # sentence tokenizer\n",
    "\n",
    "    st_list_rawfile = st(filecontentdetails)\n",
    "\n",
    "    print(len(st_list_rawfile))\n",
    "    print(st_list_rawfile)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"-------Output from assigned variable-------\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    localveriabledata = localtextvalue()\n",
    "\n",
    "    print(localveriabledata)\n",
    "\n",
    "    # sentence tokenizer\n",
    "\n",
    "    st_list_local = st(localveriabledata)\n",
    "\n",
    "    print(len(st_list_local))\n",
    "\n",
    "    print(st_list_local)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"-------Output Corpus data--------------\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    fromcorpusdata = readcorpus()\n",
    "\n",
    "    print(fromcorpusdata)\n",
    "\n",
    "    # sentence tokenizer\n",
    "\n",
    "    st_list_corpus = st(fromcorpusdata)\n",
    "\n",
    "    print(len(st_list_corpus))\n",
    "    print(st_list_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대문자에서 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming', 'is', 'funnier', 'than', 'a', 'bummer', 'says', 'the', 'sushi', 'loving', 'computer', 'scientist', '.', 'She', 'really', 'wants', 'to', 'buy', 'cars', '.', 'She', 'told', 'me', 'angrily', '.', 'It', 'is', 'better', 'for', 'you', '.', 'Man', 'is', 'walking', '.', 'We', 'are', 'meeting', 'tomorrow', '.', 'You', 'really', 'do', \"n't\", 'know..', '!']\n",
      "\n",
      "\n",
      "----------Word Lemmatization----------\n",
      "car\n",
      "walk\n",
      "meeting\n",
      "meet\n",
      "good\n",
      "\n",
      "\n",
      "----------converting data to lower case ----------\n",
      "i am a person. do you know what is time now?\n"
     ]
    }
   ],
   "source": [
    "##대문자에서 소문자로 변환\n",
    "def wordlowercase():\n",
    "\n",
    "    text=\"I am a person. Do you know what is time now?\"\n",
    "\n",
    "    print(text.lower())\n",
    "\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    wordtokenization()\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"----------Word Lemmatization----------\")\n",
    "\n",
    "    wordlemmatization()\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"----------converting data to lower case ----------\")\n",
    "\n",
    "    wordlowercase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원시 텍스트에 대한 어간(글자에서 변하지 않는 부분) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem is funnier than a bummer sav the sushi love comput scientist. she realli want to buy cars. she told me angrilv.\n"
     ]
    }
   ],
   "source": [
    "#원시 텍스트에 대한 어간(글자에서 변하지 않는 부분) 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"\"\"Stemming is funnier than a bummer savs the sushi loving computer scientist. She really wants to buy cars. She told me angrilv.\"\"\"\n",
    "\n",
    "def stemmer_porter():\n",
    "    port = PorterStemmer()\n",
    "    return \" \".join([port.stem(i) for i in text.split()])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(stemmer_porter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원시 텍스트의 표제어(중심 단어) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmer\n",
      "stem is funnier than a bummer say the sushi love comput scientist. she realli want to buy cars. she told me angrily. It is better for you. man is walking. We are meet tomorrow.\n",
      "\n",
      "Verb lemma\n",
      "Stemming be funnier than a bummer say the sushi love computer scientist. She really want to buy cars. She tell me angrily. It be better for you. Man be walking. We be meet tomorrow.\n",
      "\n",
      "Noun lemma\n",
      "Stemming is funnier than a bummer say the sushi loving computer scientist. She really want to buy cars. She told me angrily. It is better for you. Man is walking. We are meeting tomorrow.\n",
      "\n",
      "Adjective lemma\n",
      "Stemming is funny than a bummer says the sushi loving computer scientist. She really wants to buy cars. She told me angrily. It is good for you. Man is walking. We are meeting tomorrow.\n",
      "\n",
      "Satellite adjectives lemma\n",
      "Stemming is funny than a bummer says the sushi loving computer scientist. She really wants to buy cars. She told me angrily. It is good for you. Man is walking. We are meeting tomorrow.\n",
      "\n",
      "Adverb lemma\n",
      "Stemming is funnier than a bummer says the sushi loving computer scientist. She really wants to buy cars. She told me angrily. It is well for you. Man is walking. We are meeting tomorrow.\n"
     ]
    }
   ],
   "source": [
    "#원시 텍스트의 표제어(중심 단어) 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"Stemming is funnier than a bummer says the sushi loving computer scientist.\n",
    "\n",
    "She really wants to buy cars. She told me angrily.\n",
    "\n",
    "It is better for you. Man is walking. We are meeting tomorrow.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stemmer_porter():\n",
    "\n",
    "    port = PorterStemmer()\n",
    "\n",
    "    print(\"\\nStemmer\")\n",
    "\n",
    "    return \" \".join([port.stem(i) for i in text.split()])\n",
    "\n",
    "\n",
    "\n",
    "def lammatizer():\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "\n",
    "    # Pos = verb\n",
    "\n",
    "    print(\"\\nVerb lemma\")\n",
    "\n",
    "    print(\" \".join([wordnet_lemmatizer.lemmatize(i,pos=\"v\") for i in text.split()]))\n",
    "\n",
    "    # Pos =  noun\n",
    "\n",
    "    print(\"\\nNoun lemma\")\n",
    "\n",
    "    print(\" \".join([wordnet_lemmatizer.lemmatize(i,pos=\"n\") for i in text.split()]))\n",
    "\n",
    "    # Pos = Adjective\n",
    "\n",
    "    print(\"\\nAdjective lemma\")\n",
    "\n",
    "    print(\" \".join([wordnet_lemmatizer.lemmatize(i, pos=\"a\") for i in text.split()]))\n",
    "\n",
    "    # Pos = satellite adjectives\n",
    "\n",
    "    print(\"\\nSatellite adjectives lemma\")\n",
    "\n",
    "    print(\" \".join([wordnet_lemmatizer.lemmatize(i, pos=\"s\") for i in text.split()]))\n",
    "\n",
    "    print(\"\\nAdverb lemma\")\n",
    "\n",
    "    # POS = Adverb\n",
    "\n",
    "    print(\" \".join([wordnet_lemmatizer.lemmatize(i, pos=\"r\") for i in text.split()]))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(stemmer_porter())\n",
    "\n",
    "    lammatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk 기준 불용성 제거 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n"
     ]
    }
   ],
   "source": [
    "#nltk 기준 불용성 제거 리스트\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopwordlist():\n",
    "    stopwordlist = stopwords.words('english')\n",
    "    for s in stopwordlist:\n",
    "        print(s)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stopwordlist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is foo.\n"
     ]
    }
   ],
   "source": [
    "#사용자 정의 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def customizedStopWordRemove():\n",
    "    stop_words = set([\"hi\", \"bye\"])\n",
    "    line = \"\"\"hi this is foo. bye\"\"\"\n",
    "    print(\" \".join(word for word in line.split() if word not in stop_words))\n",
    "    \n",
    "customizedStopWordRemove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원시텍스트에서 불용성 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Stop word removal from raw text--------\n",
      "test sentence. happy today\n"
     ]
    }
   ],
   "source": [
    "#원시텍스트에서 불용성 제거\n",
    "def stopWordRemove():\n",
    "    stop = set(stopwords.words('english'))\n",
    "    sentence = \"this is a test sentence. I am very happy today\"\n",
    "    print(\" \".join([i for i in sentence.lower().split() if i not in stop]))\n",
    "\n",
    "stopWordRemove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 토큰화와 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming', 'is', 'funnier', 'than', 'a', 'bummer', 'says', 'the', 'sushi', 'loving', 'computer', 'scientist', '.', 'She', 'really', 'wants', 'to', 'buy', 'cars', '.', 'She', 'told', 'me', 'angrily', '.', 'It', 'is', 'better', 'for', 'you', '.', 'Man', 'is', 'walking', '.', 'We', 'are', 'meeting', 'tomorrow', '.', 'You', 'really', 'do', \"n't\", 'know..', '!']\n",
      "\n",
      "\n",
      "----------Word Lemmatization----------\n",
      "car\n",
      "walk\n",
      "meeting\n",
      "meet\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "def wordtokenization():\n",
    "\n",
    "    content = \"\"\"Stemming is funnier than a bummer says the sushi loving computer scientist.\n",
    "\n",
    "    She really wants to buy cars. She told me angrily. It is better for you.\n",
    "\n",
    "    Man is walking. We are meeting tomorrow. You really don't know..!\"\"\"\n",
    "\n",
    "    print(word_tokenize(content))\n",
    "\n",
    "\n",
    "\n",
    "def wordlemmatization():\n",
    "\n",
    "    wordlemma = WordNetLemmatizer()\n",
    "\n",
    "    print(wordlemma.lemmatize('cars'))\n",
    "\n",
    "    print(wordlemma.lemmatize('walking',pos='v'))\n",
    "\n",
    "    print(wordlemma.lemmatize('meeting',pos='n'))\n",
    "\n",
    "    print(wordlemma.lemmatize('meeting',pos='v'))\n",
    "\n",
    "    print(wordlemma.lemmatize('better',pos='a'))\n",
    "    \n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    wordtokenization()\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"----------Word Lemmatization----------\")\n",
    "\n",
    "    wordlemmatization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
