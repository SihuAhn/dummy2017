{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load breaset cancer data\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# cancer = load_breast_cancer()\n",
    "# print(dir(cancer))\n",
    "# print(cancer.DESCR)\n",
    "# print(cancer.feature_names)\n",
    "# print(cancer.data[0])\n",
    "# print(cancer.target_names)\n",
    "# print(cancer.target)\n",
    "\n",
    "# #confusion_matrix를 호출하여 결과 확인\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# y_pred = dtc.predict(X_test_fit)\n",
    "# confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e509c34b3c8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m \u001b[1;31m#sklearn datasets의 유방암 진단 데이터 호출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m#pandas를 호출하고 pd로 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m#numpy를 호출하고 np로 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[0;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                              tslib as _tslib)\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      5\u001b[0m     iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime, Period)\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_convert_single\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer #sklearn datasets의 유방암 진단 데이터 호출\n",
    "import pandas as pd #pandas를 호출하고 pd로 정의\n",
    "import numpy as np #numpy를 호출하고 np로 정의\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]],\n",
    "                    columns = np.append(cancer['feature_names'], [\"target\"]))\n",
    "\n",
    "X = data.drop([\"target\"], axis=1) #target을 제외한 feature들을 X에 저장\n",
    "y = data[\"target\"].astype(int) #target을 y에 저장\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # train data와 test data를 7:3으로 나누고 random 값 고정\n",
    "\n",
    "# features의 scale 설정\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "a = X_train\n",
    "X_train_fit = sc.fit_transform(a)\n",
    "b = X_test\n",
    "X_test_fit = sc.fit_transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-09e2ff401026>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdtc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#dtc 머신 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_fit\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#머신의 예측값\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_fit' is not defined"
     ]
    }
   ],
   "source": [
    "#DecisionTree 기반 모델\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 0) #dtc 머신 생성\n",
    "dtc.fit(X_train_fit, y_train) \n",
    "y_pred = dtc.predict(X_test_fit) #머신의 예측값\n",
    "\n",
    "# sklearn.metrics의 f1_score를 통해 DecisionTree 기반 모델 결과 확인\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"DecisionTree 기반 모델 결과 : \",end=\"\")\n",
    "print(f1_score(y_test, y_pred, average='binary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest 기반 모델\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, \n",
    "                             random_state=0)\n",
    "rfc.fit(X_train_fit, y_train)\n",
    "y_pred = rfc.predict(X_test_fit)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"RandomForest 기반 모델 결과 : \",end=\"\")\n",
    "print(f1_score(y_test, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ca4ffe5c19a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m lrc = LogisticRegression(solver='lbfgs',\n\u001b[0;32m      4\u001b[0m                          random_state=0)\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_fit' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrc = LogisticRegression(solver='lbfgs',\n",
    "                         random_state=0)\n",
    "lrc.fit(X_train_fit, y_train)\n",
    "y_pred = lrc.predict(X_test_fit)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"LinearRegression 기반 모델 결과 : \",end=\"\")\n",
    "print(f1_score(y_test, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron 기반 모델 결과 : 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "pc = Perceptron(tol=1e-3,\n",
    "                random_state=0)\n",
    "pc.fit(X_train_fit, y_train)\n",
    "y_pred = pc.predict(X_test_fit)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Perceptron 기반 모델 결과 : \",end=\"\")\n",
    "print(f1_score(y_test, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-61c8787e4220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    random_state=0)\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    975\u001b[0m         \"\"\"\n\u001b[0;32m    976\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 977\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                            incremental):\n\u001b[0;32m    341\u001b[0m             \u001b[1;31m# First time training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;31m# lbfgs does not support mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, y, layer_units)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             coef_init, intercept_init = self._init_coef(layer_units[i],\n\u001b[1;32m--> 283\u001b[1;33m                                                         layer_units[i + 1])\n\u001b[0m\u001b[0;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_init_coef\u001b[1;34m(self, fan_in, fan_out)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# Generate weights and bias:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         coef_init = self._random_state.uniform(-init_bound, init_bound,\n\u001b[1;32m--> 306\u001b[1;33m                                                (fan_in, fan_out))\n\u001b[0m\u001b[0;32m    307\u001b[0m         intercept_init = self._random_state.uniform(-init_bound, init_bound,\n\u001b[0;32m    308\u001b[0m                                                     fan_out)\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.uniform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont2_array_sc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mc = MLPClassifier(solver='lbfgs',\n",
    "                   hidden_layer_sizes=(500000, 200000),\n",
    "                   random_state=0)\n",
    "mc.fit(X_train_fit, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = mc.predict(X_test_fit)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"MLP 기반 모델 결과 : \",end=\"\")\n",
    "print(f1_score(y_test, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
